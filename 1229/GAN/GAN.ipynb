{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 下載kaggle資料集"
      ],
      "metadata": {
        "id": "zfU3NGT9GgK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_token = {\"username\":\"yanghsinyu\",\"key\":\"812b9f8db55e347f14ae82e38fb9f2e9\"}\n",
        "\n",
        "import json\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "if not os.path.exists(\"/root/.kaggle\"):\n",
        "    os.makedirs(\"/root/.kaggle\")\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "if not os.path.exists(\"/kaggle\"):\n",
        "    os.makedirs(\"/kaggle\")\n",
        "os.chdir('/kaggle')\n",
        "!kaggle datasets download -d andrewmvd/face-mask-detection"
      ],
      "metadata": {
        "id": "-kea7FIOGd1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /kaggle\n",
        "!unzip \"/kaggle/face-mask-detection\""
      ],
      "metadata": {
        "id": "t2cICxXZaBQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf '/kaggle/annotations'\n",
        "!rm -rf '/kaggle/face-mask-detection.zip'"
      ],
      "metadata": {
        "id": "GN0xtTX7aEYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAN"
      ],
      "metadata": {
        "id": "aHZhZWgFG5y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "import torchvision.datasets as dset\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch"
      ],
      "metadata": {
        "id": "wAjhLV5pGoAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model儲存路徑\n",
        "dest = '/content/gan/'\n",
        "#資料集路徑\n",
        "base_dir ='/kaggle/'\n",
        "os.makedirs(dest, exist_ok=True)\n",
        "\n",
        "#參數設定\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('-f')\n",
        "parser.add_argument(\"--n_epochs\", type=int, default=200, help=\"number of epochs of training\")\n",
        "parser.add_argument(\"--batch_size\", type=int, default=12, help=\"size of the batches\")\n",
        "parser.add_argument(\"--lr\", type=float, default=0.002, help=\"learning rate\")\n",
        "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
        "parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\n",
        "parser.add_argument(\"--img_size\", type=int, default=64, help=\"size of each image dimension\")\n",
        "parser.add_argument(\"--channels\", type=int, default=3, help=\"number of image channels\")\n",
        "parser.add_argument(\"--n_critic\", type=int, default=5, help=\"number of training steps for discriminator per iter\")\n",
        "parser.add_argument(\"--clip_value\", type=float, default=0.01, help=\"lower and upper clip value for disc. weights\")\n",
        "parser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval betwen image samples\")\n",
        "opt = parser.parse_args()\n",
        "print(opt)\n",
        "\n",
        "img_shape = (opt.channels, opt.img_size, opt.img_size)\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "if(cuda):\n",
        "  print('TURE')\n",
        "else:\n",
        "  print('FALSE')"
      ],
      "metadata": {
        "id": "Lg1i58bpG_8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generator"
      ],
      "metadata": {
        "id": "5z9-cl5fHlm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "                #(100)*1*1\n",
        "                nn.ConvTranspose2d( opt.latent_dim, 256, 3, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(256),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "                #(256)*((1-1)*1-2*0+3=3)*3\n",
        "                nn.ConvTranspose2d( 256, 128, 3, 2, 0, bias=False),\n",
        "                nn.BatchNorm2d(128),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "                #(128)*((3-1)*2-2*0+3=7)*7\n",
        "                nn.ConvTranspose2d( 128, 64, 3, 2, 0, bias=False),\n",
        "                nn.BatchNorm2d(64),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "                #(64)*((7-1)*2-2*0+3=15)*15\n",
        "                nn.ConvTranspose2d( 64, 32, 3, 2, 0, bias=False),\n",
        "                nn.BatchNorm2d(32),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "                #(32)*((15-1)*2-2*0+3=31)*31\n",
        "                nn.ConvTranspose2d( 32, opt.channels, 4, 2, 0, bias=False),\n",
        "                nn.BatchNorm2d(opt.channels),\n",
        "\n",
        "                #(3)*((31-1)*2-2*0+4=64)*64\n",
        "                nn.Tanh()\n",
        "                )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, z):\n",
        "\n",
        "        z = z.view(opt.batch_size, opt.latent_dim, 1, 1)\n",
        "        img = self.model(z)\n",
        "\n",
        "        return img"
      ],
      "metadata": {
        "id": "RrfQ2ed2Hk3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discriminator"
      ],
      "metadata": {
        "id": "2qoWWx1KHy3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.input_dim = opt.channels\n",
        "        self.output_dim = 1\n",
        "        self.input_size = int(np.prod(img_shape))\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            # input is (3) x 64 x 64\n",
        "            nn.Conv2d(opt.channels, 64, 5, 1, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # (64) x ((64-5+2*1)/1+1=62)*62\n",
        "            nn.MaxPool2d(kernel_size = 2),\n",
        "\n",
        "            # state size. (64) x 31 x 31\n",
        "            nn.Conv2d(64, 128, 3, 1, 2, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # (128) x ((31-3+2*1)/2+1=16) x 16\n",
        "            nn.MaxPool2d(kernel_size = 2),\n",
        "\n",
        "            # state size. (128) x 8 x 8\n",
        "            nn.Conv2d(128, 256, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # (256) x ((8-3+2*1)/1+1=8) x 8\n",
        "            nn.MaxPool2d(kernel_size = 2),\n",
        "\n",
        "            # state size. (256) x 4 x 4\n",
        "            nn.Conv2d(256, 1, 4, 1, 0, bias=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        validity  = self.model(img)\n",
        "        return validity"
      ],
      "metadata": {
        "id": "kC8AraypHk9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 使用GPU"
      ],
      "metadata": {
        "id": "AJgRt3lTH9Lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize generator and discriminator\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "if cuda:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()"
      ],
      "metadata": {
        "id": "3KAvbXqiH8sX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#讀取資料集"
      ],
      "metadata": {
        "id": "Z5gOQONoIGFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset = dset.ImageFolder(root=base_dir, transform=transforms.Compose([\n",
        "                               transforms.Resize(opt.img_size),\n",
        "                               transforms.CenterCrop(opt.img_size),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ])),\n",
        "    batch_size=opt.batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "print(dataloader.dataset)"
      ],
      "metadata": {
        "id": "sgKxzEWhIEOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "VhmpkNC_IPEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizers\n",
        "optimizer_G = torch.optim.RMSprop(generator.parameters(), lr=opt.lr)\n",
        "optimizer_D = torch.optim.RMSprop(discriminator.parameters(), lr=opt.lr)\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
      ],
      "metadata": {
        "id": "yr4qWudlIPMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(dest+\"model/checkpoint/\", exist_ok=True)\n",
        "# 定義要保存檢查點的路徑和檔名\n",
        "checkpoint_path = dest+'model/checkpoint/'"
      ],
      "metadata": {
        "id": "Z-s-aU9ZInl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "oYYfO2EoIvHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "batches_done = 0\n",
        "for epoch in range(opt.n_epochs):\n",
        "    for i, (imgs, _) in enumerate(dataloader):\n",
        "\n",
        "\n",
        "        # Configure input\n",
        "        real_imgs = Variable(imgs.type(Tensor))\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\n",
        "\n",
        "        # Generate a batch of images\n",
        "        fake_imgs = generator(z).detach()\n",
        "        # Adversarial loss\n",
        "        loss_D = -torch.mean(discriminator(real_imgs)) + torch.mean(discriminator(fake_imgs))\n",
        "\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Clip weights of discriminator\n",
        "        for p in discriminator.parameters():\n",
        "            p.data.clamp_(-opt.clip_value, opt.clip_value)\n",
        "\n",
        "        # Train the generator every n_critic iterations\n",
        "        if i % opt.n_critic == 0:\n",
        "\n",
        "          # -----------------\n",
        "          #  Train Generator\n",
        "          # -----------------\n",
        "\n",
        "          optimizer_G.zero_grad()\n",
        "\n",
        "          # Generate a batch of images\n",
        "          gen_imgs = generator(z)\n",
        "          # Adversarial loss\n",
        "          loss_G = -torch.mean(discriminator(gen_imgs))\n",
        "\n",
        "          loss_G.backward()\n",
        "          optimizer_G.step()\n",
        "\n",
        "          if i % 100 == 0:\n",
        "            print(\n",
        "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
        "                % (epoch+1, opt.n_epochs, batches_done % len(dataloader), len(dataloader), loss_D.item(), loss_G.item())\n",
        "            )\n",
        "\n",
        "        batches_done += 1\n",
        "\n",
        "    if (epoch+1) % 20 == 0:\n",
        "    \t  # 建立一個字典來保存模型、優化器和其他相關資訊\n",
        "        checkpoint = {\n",
        "     \t        'epoch': epoch,\n",
        "      \t        'model_G_state_dict': generator.state_dict(),\n",
        "      \t        'model_D_state_dict': discriminator.state_dict(),\n",
        "       \t        # 如果需要保存其他相關資訊，可以在此添加\n",
        "        }\n",
        "\n",
        "    \t  # 使用`torch.save`函式保存檢查點到指定的路徑\n",
        "        torch.save(checkpoint,checkpoint_path+\"checkpoint_%d.pth\" %(epoch+1))\n",
        "    save_image(gen_imgs.data[:25], dest+\"%d-%d.png\" % (epoch+1, batches_done % len(dataloader)), nrow=5, normalize=True)"
      ],
      "metadata": {
        "id": "IQD-rtfDItXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 產生亂數z"
      ],
      "metadata": {
        "id": "ianxxaudl6Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample noise as generator input\n",
        "z = Variable(Tensor(np.random.normal(0, 1, (opt.batch_size, opt.latent_dim))))\n",
        "z1 = Variable(Tensor(np.random.normal(0, 1, (opt.batch_size, opt.latent_dim))))\n",
        "z2 = Variable(Tensor(np.random.normal(0, 1, (opt.batch_size, opt.latent_dim))))\n",
        "z3 = Variable(Tensor(np.random.normal(0, 1, (opt.batch_size, opt.latent_dim))))\n",
        "z4 = Variable(Tensor(np.random.normal(0, 1, (opt.batch_size, opt.latent_dim))))\n",
        "\n",
        "print(z.size())"
      ],
      "metadata": {
        "id": "oeZYA02xl6no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 生成成圖片 ( batch_size張, 3x64x64 )"
      ],
      "metadata": {
        "id": "wUUev9LKnKOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#zn = Variable(Tensor(np.random.normal(0, 1, (opt.img_size, opt.latent_dim))))\n",
        "# Assuming generator is your PyTorch generator model\n",
        "fake_imgs = generator(z)\n",
        "fake_imgs1 = generator(z1)\n",
        "fake_imgs2 = generator(z2)\n",
        "fake_imgs3 = generator(z3)\n",
        "fake_imgs4 = generator(z4)\n",
        "print(\"fake_imgs\\n\")\n",
        "print(fake_imgs.min())\n",
        "\n",
        "# Convert generated images to numpy arrays\n",
        "fake_imgs_np = fake_imgs.cpu().detach().numpy()\n",
        "fake_imgs_np1 = fake_imgs1.cpu().detach().numpy()\n",
        "fake_imgs_np2 = fake_imgs2.cpu().detach().numpy()\n",
        "fake_imgs_np3 = fake_imgs3.cpu().detach().numpy()\n",
        "fake_imgs_np4 = fake_imgs4.cpu().detach().numpy()\n",
        "\n",
        "# Rearrange the dimensions for color images (assuming RGB)\n",
        "fake_imgs_np = np.transpose(fake_imgs_np* 0.5 + 0.5, (0, 2, 3, 1))  # from (batch_size, 3, 64, 64) to (batch_size, 64, 64, 3)\n",
        "fake_imgs_np1 = np.transpose(fake_imgs_np1* 0.5 + 0.5, (0, 2, 3, 1))  # from (batch_size, 3, 64, 64) to (batch_size, 64, 64, 3)\n",
        "fake_imgs_np2 = np.transpose(fake_imgs_np2* 0.5 + 0.5, (0, 2, 3, 1))  # from (batch_size, 3, 64, 64) to (batch_size, 64, 64, 3)\n",
        "fake_imgs_np3 = np.transpose(fake_imgs_np3* 0.5 + 0.5, (0, 2, 3, 1))  # from (batch_size, 3, 64, 64) to (batch_size, 64, 64, 3)\n",
        "fake_imgs_np4 = np.transpose(fake_imgs_np4* 0.5 + 0.5, (0, 2, 3, 1))  # from (batch_size, 3, 64, 64) to (batch_size, 64, 64, 3)\n",
        "\n",
        "\n",
        "# Define a function to display multiple images\n",
        "def show_images(images, titles):\n",
        "    num_images = len(images)\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(1, num_images, i + 1)\n",
        "        plt.imshow(images[i])\n",
        "        plt.title(titles[i])\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Display the generated images using matplotlib\n",
        "show_images([fake_imgs_np[0], fake_imgs_np1[0], fake_imgs_np2[0], fake_imgs_np3[0], fake_imgs_np4[0]], ['Image1', 'Image2', 'Image3', 'Image4', 'Image5'])"
      ],
      "metadata": {
        "id": "7v6j_JXJnHFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lode model"
      ],
      "metadata": {
        "id": "_L5174bgJRY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###讀取訓練好的模型\n",
        "checkpoint = torch.load('/content/gan/model/checkpoint/checkpoint_200.pth')\n",
        "generator.load_state_dict(checkpoint['model_G_state_dict'])\n",
        "discriminator.load_state_dict(checkpoint['model_D_state_dict'])\n",
        "start_epoch = checkpoint['epoch']"
      ],
      "metadata": {
        "id": "hKTwSm-gJUHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 生成成圖片 ( batch_size張, 3x64x64 )"
      ],
      "metadata": {
        "id": "WTp7d0W4ditQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#zn = Variable(Tensor(np.random.normal(0, 1, (opt.img_size, opt.latent_dim))))\n",
        "# Assuming generator is your PyTorch generator model\n",
        "fake_imgs = generator(z)\n",
        "fake_imgs1 = generator(z1)\n",
        "fake_imgs2 = generator(z2)\n",
        "fake_imgs3 = generator(z3)\n",
        "fake_imgs4 = generator(z4)\n",
        "print(\"fake_imgs\\n\")\n",
        "print(fake_imgs.min())\n",
        "\n",
        "# Convert generated images to numpy arrays\n",
        "fake_imgs_np = fake_imgs.cpu().detach().numpy()\n",
        "fake_imgs_np1 = fake_imgs1.cpu().detach().numpy()\n",
        "fake_imgs_np2 = fake_imgs2.cpu().detach().numpy()\n",
        "fake_imgs_np3 = fake_imgs3.cpu().detach().numpy()\n",
        "fake_imgs_np4 = fake_imgs4.cpu().detach().numpy()\n",
        "\n",
        "# Rearrange the dimensions for color images (assuming RGB)\n",
        "fake_imgs_np = np.transpose(fake_imgs_np* 0.5 + 0.5, (0, 2, 3, 1))  # from (batch_size, 3, 64, 64) to (batch_size, 64, 64, 3)\n",
        "fake_imgs_np1 = np.transpose(fake_imgs_np1* 0.5 + 0.5, (0, 2, 3, 1))  # from (batch_size, 3, 64, 64) to (batch_size, 64, 64, 3)\n",
        "fake_imgs_np2 = np.transpose(fake_imgs_np2* 0.5 + 0.5, (0, 2, 3, 1))  # from (batch_size, 3, 64, 64) to (batch_size, 64, 64, 3)\n",
        "fake_imgs_np3 = np.transpose(fake_imgs_np3* 0.5 + 0.5, (0, 2, 3, 1))  # from (batch_size, 3, 64, 64) to (batch_size, 64, 64, 3)\n",
        "fake_imgs_np4 = np.transpose(fake_imgs_np4* 0.5 + 0.5, (0, 2, 3, 1))  # from (batch_size, 3, 64, 64) to (batch_size, 64, 64, 3)\n",
        "\n",
        "\n",
        "# Define a function to display multiple images\n",
        "def show_images(images, titles):\n",
        "    num_images = len(images)\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(1, num_images, i + 1)\n",
        "        plt.imshow(images[i])\n",
        "        plt.title(titles[i])\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Display the generated images using matplotlib\n",
        "show_images([fake_imgs_np[0], fake_imgs_np1[0], fake_imgs_np2[0], fake_imgs_np3[0], fake_imgs_np4[0]], ['Image1', 'Image2', 'Image3', 'Image4', 'Image5'])"
      ],
      "metadata": {
        "id": "IzpoC94vdjXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#訓練"
      ],
      "metadata": {
        "id": "AujLo2JXm3GS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "\n",
        "#要訓練的epoch數量\n",
        "end_epoch = 5\n",
        "\n",
        "batches_done = 0\n",
        "for epoch in range(start_epoch, start_epoch+end_epoch):\n",
        "    for i, (imgs, _) in enumerate(dataloader):\n",
        "\n",
        "\n",
        "        # Configure input\n",
        "        real_imgs = Variable(imgs.type(Tensor))\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Sample noise as generator input\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\n",
        "\n",
        "        # Generate a batch of images\n",
        "        fake_imgs = generator(z).detach()\n",
        "        # Adversarial loss\n",
        "        loss_D = -torch.mean(discriminator(real_imgs)) + torch.mean(discriminator(fake_imgs))\n",
        "\n",
        "        loss_D.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Clip weights of discriminator\n",
        "        for p in discriminator.parameters():\n",
        "            p.data.clamp_(-opt.clip_value, opt.clip_value)\n",
        "\n",
        "        # Train the generator every n_critic iterations\n",
        "        if i % opt.n_critic == 0:\n",
        "\n",
        "          # -----------------\n",
        "          #  Train Generator\n",
        "          # -----------------\n",
        "\n",
        "          optimizer_G.zero_grad()\n",
        "\n",
        "          # Generate a batch of images\n",
        "          gen_imgs = generator(z)\n",
        "          # Adversarial loss\n",
        "          loss_G = -torch.mean(discriminator(gen_imgs))\n",
        "\n",
        "          loss_G.backward()\n",
        "          optimizer_G.step()\n",
        "\n",
        "          if i % 100 == 0:\n",
        "            print(\n",
        "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
        "                % (epoch+1, opt.n_epochs, batches_done % len(dataloader), len(dataloader), loss_D.item(), loss_G.item())\n",
        "            )\n",
        "\n",
        "        batches_done += 1\n",
        "\n",
        "    if (epoch+1) % 20 == 0:\n",
        "    \t  # 建立一個字典來保存模型、優化器和其他相關資訊\n",
        "        checkpoint = {\n",
        "     \t        'epoch': epoch,\n",
        "      \t        'model_G_state_dict': generator.state_dict(),\n",
        "      \t        'model_D_state_dict': discriminator.state_dict(),\n",
        "       \t        # 如果需要保存其他相關資訊，可以在此添加\n",
        "        }\n",
        "\n",
        "    \t  # 使用`torch.save`函式保存檢查點到指定的路徑\n",
        "        torch.save(checkpoint,checkpoint_path+\"checkpoint_%d.pth\" %(epoch+1))\n",
        "    save_image(gen_imgs.data[:25], dest+\"%d-%d.png\" % (epoch+1, batches_done % len(dataloader)), nrow=5, normalize=True)"
      ],
      "metadata": {
        "id": "VD04XptHlgZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 生成成圖片 ( batch_size張, 3x64x64 )"
      ],
      "metadata": {
        "id": "tYveO--cnMeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#zn = Variable(Tensor(np.random.normal(0, 1, (opt.img_size, opt.latent_dim))))\n",
        "# Assuming generator is your PyTorch generator model\n",
        "fake_imgs = generator(z)\n",
        "fake_imgs1 = generator(z1)\n",
        "fake_imgs2 = generator(z2)\n",
        "fake_imgs3 = generator(z3)\n",
        "fake_imgs4 = generator(z4)\n",
        "print(\"fake_imgs\\n\")\n",
        "print(fake_imgs.min())\n",
        "\n",
        "# Convert generated images to numpy arrays\n",
        "fake_imgs_np = fake_imgs.cpu().detach().numpy()\n",
        "fake_imgs_np1 = fake_imgs1.cpu().detach().numpy()\n",
        "fake_imgs_np2 = fake_imgs2.cpu().detach().numpy()\n",
        "fake_imgs_np3 = fake_imgs3.cpu().detach().numpy()\n",
        "fake_imgs_np4 = fake_imgs4.cpu().detach().numpy()\n",
        "\n",
        "# Rearrange the dimensions for color images (assuming RGB)\n",
        "fake_imgs_np = np.transpose(fake_imgs_np* 0.5 + 0.5, (0, 2, 3, 1))  # from (batch_size, 3, 64, 64) to (batch_size, 64, 64, 3)\n",
        "fake_imgs_np1 = np.transpose(fake_imgs_np1* 0.5 + 0.5, (0, 2, 3, 1))  # from (batch_size, 3, 64, 64) to (batch_size, 64, 64, 3)\n",
        "fake_imgs_np2 = np.transpose(fake_imgs_np2* 0.5 + 0.5, (0, 2, 3, 1))  # from (batch_size, 3, 64, 64) to (batch_size, 64, 64, 3)\n",
        "fake_imgs_np3 = np.transpose(fake_imgs_np3* 0.5 + 0.5, (0, 2, 3, 1))  # from (batch_size, 3, 64, 64) to (batch_size, 64, 64, 3)\n",
        "fake_imgs_np4 = np.transpose(fake_imgs_np4* 0.5 + 0.5, (0, 2, 3, 1))  # from (batch_size, 3, 64, 64) to (batch_size, 64, 64, 3)\n",
        "\n",
        "\n",
        "# Define a function to display multiple images\n",
        "def show_images(images, titles):\n",
        "    num_images = len(images)\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(1, num_images, i + 1)\n",
        "        plt.imshow(images[i])\n",
        "        plt.title(titles[i])\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Display the generated images using matplotlib\n",
        "show_images([fake_imgs_np[0], fake_imgs_np1[0], fake_imgs_np2[0], fake_imgs_np3[0], fake_imgs_np4[0]], ['Image1', 'Image2', 'Image3', 'Image4', 'Image5'])"
      ],
      "metadata": {
        "id": "GlTXAGIAnPI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# save model"
      ],
      "metadata": {
        "id": "CSJUxv85h6Sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 建立一個字典來保存模型、優化器和其他相關資訊\n",
        "checkpoint = {\n",
        "     \t  'epoch': epoch,\n",
        "      \t'model_G_state_dict': generator.state_dict(),\n",
        "      \t'model_D_state_dict': discriminator.state_dict(),\n",
        "       \t# 如果需要保存其他相關資訊，可以在此添加\n",
        "}\n",
        "\n",
        "# 使用`torch.save`函式保存檢查點到指定的路徑\n",
        "torch.save(checkpoint,checkpoint_path+\"checkpoint_%d.pth\" %(epoch+1))"
      ],
      "metadata": {
        "id": "9NZs-359h6aZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}